{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow NLP cheat sheet\n",
    "> Some tips for tensorflow and keras in Natural Language Processing\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [tensorflow; NLP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ghtop_images/header2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Implementation\n",
    "\n",
    "The standard language model starts with an embedding layer, this then needs to be flattened to a vector, then we can add a dense layer before an output layer.\n",
    "\n",
    "The [`Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer creates a vector-space for the text data. So for example, the words beautiful and ugly may be in opposite directions. And words such as cat and kitten may be close together in vector space.\n",
    "\n",
    "`GlobalAveragePooling1D`can be replaced by `Flatten()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([ \n",
    "    tf.keras.layers.Embedding(num_words,embedding_dim,input_length=maxlen),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24,activation='relu'),\n",
    "    tf.keras.layers.Dense(5,'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model above does not take account for the order of words, \n",
    "\n",
    "If we want to do this we can insert an additional layer after the embedding layer. For example, by using the LSTM model as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model_lstm= tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE,EMBEDDING_DIM,input_length=MAXLEN),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(24,activation='relu'),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES,activation='softmax')   \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even insert a conolution layer after the embedding instead\n",
    "\n",
    "`tf.keras.layers.Conv1D(128,5,activation='relu')`\n",
    "\n",
    "\n",
    "For two consecutive layers of RNNs use `return_sequences=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm1_dim, return_sequences=True)),\n",
    "tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm2_dim)),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text data Tokenizer\n",
    "\n",
    "- Create a `Tokenizer` instance\n",
    "- Fit tokenizer to text data with `tokenizer.fit_on_texts(text_data)`\n",
    "- Convert text to sequences with `sequences = tokenizer.texts_to_sequences(text_data)`\n",
    "    - For example, the following words have the indices: apple->1, brain->2, cat->3, that->4, is->5\n",
    "    - And a sequence of text within the data can be converted to a sequence: \"that cat apple is brain\" -> (4, 3, 1, 5, 2)\n",
    "- Get the word index `word_index = tokenizer.word_index`\n",
    "- Get the text back from the sequences `text = tokenizer.sequences_to_texts(sequences)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\",num_words=10_000)\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "\n",
    "sequences = label_tokenizer.texts_to_sequences(text_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
