{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP examples\n",
    "> Using Python and different NLP techniques to predict disatser tweets\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [NLP, fastai, zero-shot, scikit learn, bag of words, Python, kaggle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ghtop_images/header2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Competition Description](https://www.kaggle.com/competitions/nlp-getting-started)\n",
    "\n",
    "Twitter has become an important communication channel in times of emergency.\n",
    "The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n",
    "\n",
    "But, it’s not always clear whether a person’s words are actually announcing a disaster. \n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "You'll need train.csv, test.csv and sample_submission.csv.\n",
    "What should I expect the data format to be?\n",
    "\n",
    "Each sample in the train and test set has the following information:\n",
    "\n",
    "- The text of a tweet\n",
    "- A keyword from that tweet (although this may be blank!)\n",
    "- The location the tweet was sent from (may also be blank)\n",
    "\n",
    "What am I predicting?\n",
    "\n",
    "You are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n",
    "Files\n",
    "\n",
    "- `train.csv` - the training set\n",
    "- `test.csv` - the test set\n",
    "- `sample_submission.csv` - a sample submission file in the correct format\n",
    "\n",
    "Columns\n",
    "\n",
    "- `id` - a unique identifier for each tweet\n",
    "- `text` - the text of the tweet\n",
    "- `location` - the location the tweet was sent from (may be blank)\n",
    "- `keyword` - a particular keyword from the tweet (may be blank)\n",
    "- `target` - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis methods used\n",
    "\n",
    "Code used for the threee methods:\n",
    "- https://www.kaggle.com/code/thomassimm/nlp-disaster-tweets-one-shot\n",
    "- https://www.kaggle.com/code/thomassimm/nlp-disaster-scikit\n",
    "- https://www.kaggle.com/code/thomassimm/nlp-disaster-nn\n",
    "\n",
    "### [one-shot](https://www.kaggle.com/code/thomassimm/nlp-disaster-tweets-one-shot)\n",
    "\n",
    "For the [zero shot](https://joeddav.github.io/blog/2020/05/29/ZSL.html) learning method, the main advantage is no labelled data is needed nor any prior training. But the accuracy will be reduced without any training.\n",
    "\n",
    "    What is zero-shot learning?\n",
    "\n",
    "    Traditionally, zero-shot learning (ZSL) most often referred to a fairly specific type of task: learn a classifier on one set of labels and then evaluate on a different set of labels that the classifier has never seen before. Recently, especially in NLP, it's been used much more broadly to mean get a model to do something that it wasn't explicitly trained to do. A well-known example of this is in the GPT-2 paper where the authors evaluate a language model on downstream tasks like machine translation without fine-tuning on these tasks directly.\n",
    "    https://joeddav.github.io/blog/2020/05/29/ZSL.html\n",
    "    \n",
    "The code used here is as follows:\n",
    "\n",
    "`from transformers import pipeline`\n",
    "\n",
    "`classifier = pipeline(\"zero-shot-classification\", device=0)`\n",
    "\n",
    "And can then be called as follows, where string is what we are classifying (ie. the tweet) and label what we are looking to classify it as (here we use disaster):\n",
    "\n",
    "`classout=classifier( TEXT, LABEL, multi_class=True)`\n",
    "\n",
    "\n",
    "> The accuracy is ~60%. But remember no training is done.\n",
    "\n",
    "\n",
    "### [Bag of words sci-kit learn](https://www.kaggle.com/code/thomassimm/nlp-disaster-scikit)\n",
    "\n",
    "\n",
    "This method uses scikit-learn to classify the tweets by using a *Bag of Words* approach. This example uses a Tf-idf-weighted document-term sparse matrix to encode the features and demonstrates various classifiers that can efficiently handle sparse matrices.\n",
    "\n",
    "Methodology taken from [here](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py)\n",
    "\n",
    "|Method |fit_time |\tscore_time |\ttest_accuracy |\ttrain_accuracy |\ttest_f1 |\ttrain_f1|\n",
    "|-------|---------|-----------|-----------------|-----------------|-----------|------------------|\n",
    "|Logistic Regression| \t2.349514 \t|0.006580 |\t0.739398 |\t0.977276 \t|0.639793 |\t0.973091|\n",
    "|Ridge Classifier |\t0.079390 | 0.003698 |\t0.736246 |\t0.979509 |\t0.631187 |\t0.975799|\n",
    "|kNN \t|0.004747 \t|0.636052| \t0.720220 |\t0.775220 |\t0.601846 |\t0.692391|\n",
    "|Random Forest |\t50.920274| \t0.207107 |\t0.691847 |\t0.988999 |\t0.500637 |\t0.987122|\n",
    "|Linear SVC \t|0.093184 |\t0.004035 |\t0.733750 |\t0.900926 |\t0.605876 |\t0.872385|\n",
    "|log-loss SGD \t|0.002576 |\t0.000000 |\tNaN| \tNaN |\tNaN \t|NaN|\n",
    "|NearestCentroid |\t0.008251 |\t0.005659 |\t0.664523 |\t0.772659 |\t0.658099 |\t0.751426|\n",
    "|Complement naive Bayes |\t0.008391 |\t0.003308 |\t0.690007 \t|0.979115 |\t0.676552 |\t0.975514|\n",
    "\n",
    "> Results give ~80% on results file\n",
    "\n",
    "\n",
    "### [NN with fastai](https://www.kaggle.com/code/thomassimm/nlp-disaster-nn)\n",
    "\n",
    "The method used here is to do a two step process both of which use a RNN neural network with AWD-LSTM architecture:\n",
    "\n",
    "- creating a language model for the data\n",
    "- using this language model with labelled data to classify different texts\n",
    "\n",
    "This method is shown here using [fastai](https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb) for sentiment analysis (i.e. are reviews positive or negative) of IMDB data.\n",
    "\n",
    "The basics of the code for the classifier part are:\n",
    "\n",
    "create a data loader:\n",
    "\n",
    "`dls = TextDataLoaders.from_df(df, text_col='text', label_col='target', \n",
    "                              valid_col='is_valid')`\n",
    "\n",
    "create a learner, using text_classifier and AWD_LSTM\n",
    "\n",
    "`learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5)`\n",
    "\n",
    "`learn.metrics = [FBeta(beta=1),accuracy]`\n",
    "\n",
    "And finally add the language model learnt on the tweets in the 1st step\n",
    "\n",
    "`learn = learn.load_encoder('finetuned3')`\n",
    "\n",
    "Then learn by:\n",
    "\n",
    "`learn.fit_one_cycle(2, 1e-2)`\n",
    "\n",
    "\n",
    "What surprised me was how low the final accuracy was given the complexity of the model and the time taken (over 8 mins). Maybe some work needs to be done to look at how I implemented this or try other RNN methods?\n",
    "\n",
    "> Results give about 77% accuracy on results file\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
