{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Golf Swing Part III- Using pre-trained models- FiftyOne and Streamlit App\n",
    "> Using Python and pre-trained to identify parts of the body and club during a golf swing. Uses both FiftyOne and creates an app using Streamlit\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Neural networks,FiftyOne, Golf Swing, Python]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ghtop_images/header2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In a previous part [Part 1](https://thomashsimm.com/jupyter/2021/12/01/GolfPos1FastAI.html) a neural network model was used to find positions on the body during a golf swing. The model was not particularly succesful, perhaps due to the lack of data (specific to the golf swing) that was used to train the model on.\n",
    "\n",
    "This problem can be got around by using a model that has been pre-trained on human gestures. Several pre-trained models can be found here [Pre-trained models](https://voxel51.com/docs/fiftyone/user_guide/model_zoo/models.html). I tried a few and found the chose the model `keypoint-rcnn-resnet50-fpn-coco-torch` worked well with this data. [Link to model](https://voxel51.com/docs/fiftyone/user_guide/model_zoo/models.html#keypoint-rcnn-resnet50-fpn-coco-torch) and [Paper of model](https://arxiv.org/abs/1603.00502).\n",
    "\n",
    "\n",
    "The input to the model is taken from [Part 2](https://thomashsimm.com/neural%20networks/pytourch/golf%20swing/python/2022/02/26/GolfSwingPart2.html) which separated a golf video into a series of images of the swing.\n",
    "\n",
    "In this page I use the model with both [fiftyOne](https://voxel51.com/) and as a [streamlit app](https://share.streamlit.io/dmaterialia/golfswingstreamlit/main/GolfSwingApp.py).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> youtube: https://youtu.be/Q0BB0huWb6s\n",
    "https://youtu.be/Q0BB0huWb6s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "The code can be run on google colab here [COCO50_1](https://colab.research.google.com/drive/17Bb8n9D9_4aAuJlCf2yOQE08L7vjUtBm?usp=sharing) (works best on google chrome)\n",
    "\n",
    "## Installs and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "!pip uninstall opencv_python_headless\n",
    "\n",
    "!pip install opencv-python-headless==4.5.4.60\n",
    "\n",
    "!pip install fiftyone\n",
    "\n",
    "import fiftyone as fo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upoad some images to the workspace\n",
    "\n",
    "Untar and create a dataset object from them\n",
    "\n",
    "And look at them\n",
    "\n",
    "The image files can be found here [GC2.tgz](https://drive.google.com/file/d/1FMOP3Lm-0Ed1SmuPYdcm6emUu-Jhv2CW/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "my_tar = tarfile.open('/content/GC2.tgz')\n",
    "my_tar.extractall('/content/my_folder') # specify which folder to extract to\n",
    "my_tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "import fiftyone as fo\n",
    "\n",
    "name = \"my_folder\"\n",
    "dataset_dir = \"/content\"\n",
    "\n",
    "# Create the dataset\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fo.types.ImageDirectory,\n",
    "    name=name,\n",
    ")\n",
    "\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ghtop_images/fiftyOneScreen.png)\n",
    "\n",
    "\n",
    "This screen is interative and allows us to look at the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the trained model \n",
    "\n",
    "Apply the model to the dataset\n",
    "\n",
    "View the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model = foz.load_zoo_model(\"keypoint-rcnn-resnet50-fpn-coco-torch\")\n",
    "\n",
    "# label_types=[\"classification\", \"classifications\", \"detections\", \"instances\", \"segmentations\", \"keypoints\", \"polylines\", \"polygons\", \"scalar\"],\n",
    "\n",
    "dataset.apply_model(model, label_field=\"predictions\",label_types='predictions_keypoints')\n",
    "\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ghtop_images/foo2.png)\n",
    "\n",
    "> youtube: https://youtu.be/dkxtOBWD7Vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from the model\n",
    "\n",
    "We might want to use the data from the model outside of fiftyOne. \n",
    "\n",
    "In the following I extract the data so that it can be plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def plotPredOne(i):\n",
    "  import numpy as np\n",
    "\n",
    "  import matplotlib.pyplot as plt\n",
    "  import matplotlib.image as mpimg\n",
    "\n",
    "  img = mpimg.imread(i['filepath'])\n",
    "\n",
    "  #need to take account of more than one person in image\n",
    "  points1 = np.array(i['predictions_keypoints']['keypoints'][0]['points'])\n",
    "  adjPts = np.shape(img)[0]\n",
    "  box1 = np.array(i['predictions_detections']['detections'][0]['bounding_box']) \n",
    "  box1=box1*adjPts\n",
    "  # Bboxes are in [top-left-x, top-left-y, width, height] format\n",
    "  box2=np.array([ \n",
    "      [box1[0], box1[1]],\n",
    "      [box1[0] +box1[2] ,box1[1] ],\n",
    "      [box1[0] +box1[2] ,box1[1] +box1[3]] ,\n",
    "      [box1[0]  ,box1[1] +box1[3]],\n",
    "      [box1[0], box1[1]]\n",
    "      ])\n",
    " \n",
    "  plt.figure()\n",
    "  plt.imshow(img)\n",
    "\n",
    "  plt.plot(points1[:,0]*adjPts,points1[:,1]*adjPts, '+k',markersize=10,linewidth=3)\n",
    "  plt.plot(box2[:,0],box2[:,1], '--og',markersize=10,linewidth=3)\n",
    "\n",
    "    #back of body\n",
    "  v=[4,6,12,14,16]\n",
    "  plt.plot(points1[v,0]*adjPts,points1[v,1]*adjPts, '-k<',markersize=10,linewidth=2)\n",
    "\n",
    "  #front of body\n",
    "  v=[0,5,11,13,15]\n",
    "  plt.plot(points1[v,0]*adjPts,points1[v,1]*adjPts, '-w>',markersize=10,linewidth=2)\n",
    "\n",
    "  vects = np.array([[ 5,6],#shoulders also 4?\n",
    "         [11,12], #hips\n",
    "         [13,14], #knees\n",
    "         [15,16],#heels\n",
    "         [7,8],#elbows\n",
    "         [9,10],#hands\n",
    "         ]) \n",
    "  mak='gcyrmb'\n",
    "  for iv,v in enumerate(vects):\n",
    "    plt.plot(points1[v,0]*adjPts,points1[v,1]*adjPts, '-'+mak[iv],markersize=10,linewidth=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "for iii,i in enumerate(dataset):\n",
    "  \n",
    "  plotPredOne(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ghtop_images/foo3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert into a Streamlit App\n",
    "\n",
    "To convert to a streamlit app I will use the PyTorch module rather than the fiftyOne.\n",
    "\n",
    "I will also keep it simple by loading only 3 images- start of swing, top of backswing and at impact- and modeling these at the start of the load part of the app.\n",
    "\n",
    "The app will then just plot the images as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and give the app a title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import tarfile\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "st.title('Golf Swing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data & applying the model\n",
    "\n",
    "Create a function to load data and model the data\n",
    "\n",
    "`load_data(choi)`\n",
    "\n",
    "Images are loaded from Gc2.tgz \n",
    "\n",
    "`my_tar = tarfile.open(cda2+'/GC2.tgz')` \n",
    "\n",
    "The particular model to use is loaded\n",
    "\n",
    "`model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True)`\n",
    "\n",
    "`model.eval()`\n",
    "\n",
    "Images are loaded and converted to a tensor\n",
    "\n",
    "`number_img = Image.open(cda2+'/images/'+image_filename)`\n",
    "\n",
    "`convert_tensor = transforms.ToTensor()`\n",
    "\n",
    "And predictions are made\n",
    "\n",
    "`predictions=model(imgTens)`\n",
    "\n",
    "In the main body the function is called\n",
    "\n",
    "`data_load_state = st.text('Loading data...')`\n",
    "\n",
    "`predictions,imgLocAll,cda2=load_data(1)`\n",
    "\n",
    "`data_load_state.text(\"Loaded data (using st.cache)\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# So only have to do this when app launches\n",
    "@st.cache()\n",
    "\n",
    "# the function 'choi' is the video file to use\n",
    "def load_data(choi):\n",
    "\n",
    "    # the images are in the GC2.tgz file- this needs to be untarred first\n",
    "    cda = os.getcwd()\n",
    "    cda2=cda\n",
    "    my_tar = tarfile.open(cda2+'/GC2.tgz')\n",
    "    my_tar.extractall(cda2) # specify which folder to extract to\n",
    "    my_tar.close()\n",
    "\n",
    "    # Create a variable of the image names and which video they are part of\n",
    "    imgAll=[]\n",
    "    vidAll=[]\n",
    "    i=0\n",
    "    last1=' '\n",
    "    for xx in os.listdir(cda2+'/images/'):\n",
    "        if xx[-1]=='g':\n",
    "            imgAll = np.append(imgAll, xx)\n",
    "            if xx.split('_')[1]!=last1:\n",
    "                i=i+1\n",
    "            vidAll=np.append(vidAll,i)\n",
    "            last1=xx.split('_')[1]\n",
    "\n",
    "    vidAllUnq=np.unique(vidAll)\n",
    "    \n",
    "    # Load the model to be used\n",
    "    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True)\n",
    "    model.eval()\n",
    "    \n",
    "    # Select the images to be used\n",
    "    imgs = imgAll[vidAll==vidAllUnq[choi]]\n",
    "\n",
    "    # make sure in correct order\n",
    "    aa=[int(xx.split('_')[-1].split('e')[1].split('.')[0]) for xx in imgs]\n",
    "    ind=sorted(range(len(aa)), key=lambda k: aa[k])\n",
    "    imgs=imgs[ind]\n",
    "\n",
    "    # create tensor of images to be used- here 3 (images) X width X height\n",
    "    imgTens=[]\n",
    "    imgLocAll=[]\n",
    "    \n",
    "    # Just use the start, top and impact of swing\n",
    "    iiUse=[0,3,5]\n",
    "    for ii,image_filename in enumerate(imgs):\n",
    "    #             print(cda2+'images/'+image_filename)\n",
    "        if ii in iiUse:\n",
    "            number_img = Image.open(cda2+'/images/'+image_filename)\n",
    "            convert_tensor = transforms.ToTensor()\n",
    "            number_img=convert_tensor(number_img)\n",
    "            imgTens.append(number_img)\n",
    "            imgLocAll.append(image_filename)\n",
    "\n",
    "    # Make the predictions\n",
    "    predictions=model(imgTens)\n",
    "    \n",
    "    return predictions,imgLocAll,cda2\n",
    "\n",
    "# Outside the function, the load function is called\n",
    "data_load_state = st.text('Loading data...')\n",
    "predictions,imgLocAll,cda2=load_data(1)\n",
    "data_load_state.text(\"Loaded data (using st.cache)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streamlit user interface\n",
    "\n",
    "User selects the images from this box:\n",
    "\n",
    "`choice=imgLocAll`\n",
    "\n",
    "`imgSEL = st.sidebar.selectbox(\n",
    "    'Select how to search',\n",
    "     choice)`\n",
    "     \n",
    " Display to user what swing it is:\n",
    " \n",
    "`SwingPos=['Start','Back','Through']`\n",
    "\n",
    "`SwingPos[numSEL]`\n",
    "     \n",
    " And at the end of the file the figure is displayed in streamlit with the following command:\n",
    " \n",
    " `st.pyplot(fig)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The plot part\n",
    "\n",
    "Extract the data from the model about different parts of the body:\n",
    "\n",
    "`points1=np.array([x.detach().numpy()[0:2] for x in predictions[numSEL]['keypoints'][0]])`\n",
    "\n",
    "The plot lines plot different parts of the body, the following plot the back of the body\n",
    "\n",
    "`v=[4,6,12,14,16]`\n",
    "\n",
    "`plt.plot(points1[v,0]*adjPts,points1[v,1]*adjPts, '-w<',markersize=10,linewidth=2)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# load the images so can be plotted\n",
    "img = mpimg.imread(cda2+'/images/'+imgSEL)\n",
    "\n",
    "# the image selected\n",
    "numSEL=[oo for oo,x in enumerate(choice) if x==imgSEL][0]\n",
    "\n",
    "# get data from model as a numpy array - here want keypoints other info is also available\n",
    "points1=np.array([x.detach().numpy()[0:2] for x in predictions[numSEL]['keypoints'][0]])\n",
    "\n",
    "# create a plot\n",
    "fig=plt.figure(figsize=(7,7))\n",
    "plt.imshow(img)\n",
    "\n",
    "# Plot across back and front of body\n",
    "adjPts=1\n",
    "#back of body\n",
    "v=[4,6,12,14,16]\n",
    "plt.plot(points1[v,0]*adjPts,points1[v,1]*adjPts, '-w<',markersize=10,linewidth=2)\n",
    "\n",
    "#front of body\n",
    "v=[0,5,11,13,15]\n",
    "plt.plot(points1[v,0]*adjPts,points1[v,1]*adjPts, '-k>',markersize=10,linewidth=2)\n",
    "\n",
    "# Plot over lines on body\n",
    "vects = np.array([[ 5,6],#shoulders also 4?\n",
    "     [11,12], #hips\n",
    "     [13,14], #knees\n",
    "     [15,16],#heels\n",
    "     [7,8],#elbows\n",
    "     [9,10],#hands\n",
    "     ]) \n",
    "mak='gcyrmb'\n",
    "for iv,v in enumerate(vects):\n",
    "    plt.plot(points1[v,0]*adjPts,points1[v,1]*adjPts, '-'+mak[iv],markersize=10,linewidth=3)\n",
    "\n",
    "LEG=['Back','Front','Shoulders','Hips','Knees','Heels','Elbows','Hands']\n",
    "plt.legend(LEG)\n",
    "for x in points1:\n",
    "    plt.plot(x[0],x[1],'+b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements.txt\n",
    "\n",
    "Finally streamlit needs a requirements text in the GitHub repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "torch\n",
    "torchvision\n",
    "Pillow\n",
    "matplotlib\n",
    "numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Streamlit App\n",
    "\n",
    "\n",
    "[Streamlit App](https://share.streamlit.io/dmaterialia/golfswingstreamlit/main/GolfSwingApp.py)\n",
    "\n",
    "\n",
    "[GitHub page](https://github.com/dMaterialia/GolfSwingStreamlit/)\n",
    "\n",
    "> youtube: https://youtu.be/Q0BB0huWb6s\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
