{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow cheat sheet\n",
    "> Some tips for tensorflow and keras\n",
    "\n",
    "- toc: false \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [tensorflow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ghtop_images/header2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "1. [Overview](https://thomashsimm.com/tensorflow/2022/09/28/Tensorflow.html#Overview)\n",
    "2. [General TensorFlow Procedure](https://thomashsimm.com/tensorflow/2022/09/28/Tensorflow.html#General-TensorFlow-Procedure)\n",
    "3. Lower-Level API\n",
    "    - [Functional API](https://thomashsimm.com/tensorflow/2022/09/28/Tensorflow.html#Functional-API)\n",
    "    - [Custom Layers, Models and Optimization](https://thomashsimm.com/tensorflow/low-level/2022/10/10/Tensorflow-CustomLayersModels.html)\n",
    "4. [Datasets, Tensors, Gradient Tapes and Advanced Optimization](https://thomashsimm.com/tensorflow/2022/10/07/Tensorflow-2.html)\n",
    "5. [Images](https://thomashsimm.com/tensorflow/images/2022/10/10/Tensorflow-Images.html)\n",
    "6. [NLP](https://thomashsimm.com/tensorflow;%20nlp/2022/10/10/Tensorflow-NLP.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "> This cheat-sheet is spilt over multiple pages and as I am still learning the material (October 2022) the nature and content will change when I get the time. \n",
    "\n",
    "TensorFlow is from [Wikipedia](https://en.wikipedia.org/wiki/TensorFlow): *a free and open-source software library for machine learning and artificial intelligence. It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks.*\n",
    "\n",
    "What I really like in tensorflow (v2) is the levels of knowledge/control you can have to use it from *ultra-easy* to *wtf*. The incorporation of keras is a big part of it, as it is the easiest to use NN library. But to get more control is not that much harder. \n",
    "\n",
    "\n",
    "# Useful Links / Details\n",
    "\n",
    "## General\n",
    "\n",
    "Library websites:\n",
    "\n",
    "https://keras.io/\n",
    "\n",
    "https://www.tensorflow.org/\n",
    "\n",
    "## Courses\n",
    "\n",
    "I have tried both of the following courses: DeepLearning.AI TensorFlow Developer Professional Certificate and TensorFlow 2 for Deep Learning Specialization.\n",
    "\n",
    "- [DeepLearning.AI TensorFlow Developer Professional Certificate](https://www.coursera.org/professional-certificates/tensorflow-in-practice)\n",
    "\n",
    "- [TensorFlow 2 for Deep Learning Specialization](https://www.coursera.org/specializations/tensorflow2-deeplearning)\n",
    "\n",
    "The Tensorflow2 course is a bit longer and goes into more depth, although there are additional extended courses for the deeplearning one. The deeplearning one can be done within the current 7 days trail period of coursera. The Tensorflow2 course is tricky to do in this timeframe. This is due to more material, the harder coursework, and waiting for capstone projects to be marked.\n",
    "\n",
    "In the end I only did the first course of Tensorflow2 as I found the tests had material that wasn't explained within the course and I found the lectures lacking in detail and the instructors became increasingly boring. I gave up after getting to the capstone in course 2 (of 3) when they asked a question about an NLP network that was never explained anywhere. However, the coursework is a good challenge, so it may be worth doing the course for this alone and learning from other sources in addition to this one.\n",
    "\n",
    "I prefered the DeepLearning courses as they were more in depth and didn't assume as much prior knowledge, and the presentation was better. I am currently working through the follow-up course [TensorFlow: Advanced Techniques Specialization](https://www.coursera.org/specializations/tensorflow-advanced-techniques) and given time will do the NLP and MLOps courses.\n",
    "\n",
    "\n",
    "**Some Others:**\n",
    "\n",
    "One by Udacity [Intro to TensorFlow for Deep Learning](https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187) is being offered for free and looks okay too.\n",
    "\n",
    "TensorFlow example tutorials written as Jupyter notebooks and run directly in Google Colab—a hosted notebook environment that requires no setup. Click the Run in Google Colab button. Part of the [TensorFlow resources](https://www.tensorflow.org/tutorials).\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [TensorFlow's website recommendations](https://www.tensorflow.org/resources/learn-ml)\n",
    "\n",
    "- [François Chollet](https://fchollet.com/)\n",
    "    - His book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition) can be read online.\n",
    "\n",
    "- [Deep learning book by Ian Goodfellow and Yoshua Bengio and Aaron Courville]( https://www.deeplearningbook.org/)\n",
    "    - available free online\n",
    "\n",
    "- [Probabilistic Programming & Bayesian Methods for Hackers](https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/)\n",
    "    - available online\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General TensorFlow Procedure\n",
    "\n",
    "- preprocessing the data\n",
    "- create model \n",
    "    - e.g. `model = tf.keras.Sequential([ ....])`\n",
    "- compile the model\n",
    "    - e.g. `model.compile(optimizer='adam',loss='mae')`\n",
    "- fit the model\n",
    "    - e.g. `history = model.fit(xdata, ydata)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creations\n",
    "\n",
    "The easiest way to create a model in Keras is through `keras.Sequential`, which creates a neural network as a stack of layers. \n",
    "\n",
    "So in the examplel below\n",
    "- the 1st layer has `units` = 4 and `input_shape`=2 with a relu activation function\n",
    "- the 2nd layer has `units` = 3 with a relu activation function\n",
    "- the 3rd layer has `units` = 1\n",
    "\n",
    "The 3rd layer is the output layer. Since there is no activation function this would be a regression problem to predict one value.\n",
    "\n",
    "![](https://i.imgur.com/Y5iwFQZ.png)\n",
    "\n",
    "For non sequential models or with multiple inputs/outputs see [functional API](https://thomashsimm.com/tensorflow/2022/09/28/Tensorflow.html#Functional-API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Data\n",
    "\n",
    "https://www.kaggle.com/code/thomassimm/premier-league-predictions-using-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(512//8,activation='relu',input_shape=[input_shape]),\n",
    "        tf.keras.layers.Dense(512//8,activation='relu'),\n",
    "        tf.keras.layers.Dense(1,activation='sigmoid')        \n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([ \n",
    "          tf.keras.layers.Convolution2D( 64,(3,3),activation='relu',input_shape=(28,28,1) ),\n",
    "          tf.keras.layers.MaxPool2D(2,2),\n",
    "          tf.keras.layers.Flatten(),\n",
    "          tf.keras.layers.Dense(256//2,activation='relu'),\n",
    "          tf.keras.layers.Dense(1,activation='sigmoid') ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Data\n",
    "\n",
    "The standard language model starts with an embedding layer, this then needs to be flattened to a vector, then we can add a dense layer before an output layer.\n",
    "\n",
    "The [`Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer creates a vector-space for the text data. So for example, the words beautiful and ugly may be in opposite directions. And words such as cat and kitten may be close together in vector space.\n",
    "\n",
    "`GlobalAveragePooling1D`can be replaced by `Flatten()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "    model = tf.keras.Sequential([ \n",
    "        tf.keras.layers.Embedding(num_words,embedding_dim,input_length=maxlen),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(24,activation='relu'),\n",
    "        tf.keras.layers.Dense(5,'softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model above does not take account for the order of words, \n",
    "\n",
    "If we want to do this we can insert an additional layer after the embedding layer. For example, by using the LSTM model as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model_lstm= tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE,EMBEDDING_DIM,input_length=MAXLEN),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(24,activation='relu'),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES,activation='softmax')   \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even insert a conolution layer after the embedding instead\n",
    "\n",
    "`tf.keras.layers.Conv1D(128,5,activation='relu')`\n",
    "\n",
    "\n",
    "For two consecutive layers of RNNs use `return_sequences=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm1_dim, return_sequences=True)),\n",
    "tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm2_dim)),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model\n",
    "\n",
    "To compile the model, need to define the following:\n",
    "\n",
    "- the optimizer to use, i.e. how to update the parameters to improve model during fitting\n",
    "- the loss, i.e. what defines the goodness of the fit\n",
    "- any metrics to record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Evaluate` / `Predict` the model\n",
    "\n",
    "`test_loss, test_accuracy = model.evaluate(scaled_test_images,`\n",
    "\n",
    "`tf.keras.utils.to_categorical(test_labels),`\n",
    "                  \n",
    "`verbose=0)`\n",
    "\n",
    "and more or less outputs depending on metrics used\n",
    "\n",
    "`pred = model.predict(X_sample)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading\n",
    "\n",
    "### Saving / Loading weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model_weights_file='my_file'\n",
    "\n",
    "model.save_weights(model_weights_file)\n",
    "\n",
    "model.load_weights(model_weights_file)\n",
    "\n",
    "\n",
    "# All the model\n",
    "\n",
    "model.save('saved_model/my_model')\n",
    "\n",
    "new_model = tf.keras.models.load_model('saved_model/my_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "within `model.fit(....)`\n",
    "\n",
    "`callbacks=[callback_function]`\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/callbacks\n",
    "\n",
    "\n",
    "- `EarlyStopping`\n",
    "    - to stop the model early if some conditions are met\n",
    "- `ModelCheckpoint`\n",
    "    - save the model/model weights (https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint)\n",
    "    - main data stored similar to '.data-00000-of-00001'\n",
    "    - https://www.tensorflow.org/tutorials/keras/save_and_load#what_are_these_files\n",
    "    - Can give file names with variables using {}\n",
    "\n",
    "\n",
    "- `val_loss`\n",
    "- `val_accuracy`\n",
    "- `batch`\n",
    "- `epoch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    min_delta=0.0001, # minimium amount of change to count as an improvement\n",
    "    patience=20, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    "    monitor='val_binary_accuracy',\n",
    ")\n",
    "\n",
    "filepath = os.path.join(cwd,'checkpoints_every_epoch/checkpoint.{epoch}.{batch}')\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        filepath=filepath,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout \n",
    "\n",
    "The idea behind dropout is to randomly drop out some fraction of a layer's input units every step of training, making it much harder for the network to learn those spurious patterns in the training data that leads to overfitting. \n",
    "\n",
    "Instead, it has to search for broad, general patterns, whose weight patterns tend to be more robust.\n",
    "\n",
    "You could also think about dropout as creating a kind of ensemble of networks like with RandomForests.\n",
    "\n",
    "Example useage, apply 30% dropout to the next layer\n",
    "\n",
    "`layers.Dropout(rate=0.3),`\n",
    "\n",
    "`layers.Dense(16)`\n",
    "\n",
    "Example taken from kaggle https://www.kaggle.com/code/ryanholbrook/dropout-and-batch-normalization\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/a86utxY.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "Normalization is important in neural networks, it can really significantly improve the results of the values of the input and output are between 0 and 1.\n",
    "\n",
    "In contrast, it is not important in other models such as random forests. Hence, the input data is often normalised such as `train_datagen = ImageDataGenerator(rescale=1./255.)` in image models.\n",
    "\n",
    "So if it is good to normalize the input data it can also be good to normalize the layers of the network. This can be done with a `BatchNormalization` layer.A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then also putting the data on a new scale with two trainable rescaling parameters. Batchnorm, in effect, performs a kind of coordinated rescaling of its inputs.\n",
    "\n",
    "As stated in  https://www.kaggle.com/code/ryanholbrook/dropout-and-batch-normalization batchnorm:\n",
    "- batchnorm is most often added as an aid to the optimization process \n",
    "- but it can sometimes also help prediction performance\n",
    "- Models with batchnorm tend to need fewer epochs to complete training. \n",
    "- And can  fix various problems that can cause the training to get \"stuck\". \n",
    "- it can be used at almost any point in a network. \n",
    "\n",
    "`layers.Dense(16, activation='relu'),`\n",
    "\n",
    "`layers.BatchNormalization(),`\n",
    "\n",
    "... or between a layer and its activation function:\n",
    "\n",
    "`layers.Dense(16),`\n",
    "\n",
    "`layers.BatchNormalization(),`\n",
    "\n",
    "`layers.Activation('relu'),`\n",
    "\n",
    "... Or if you add it as the first layer of your network it can act as a kind of adaptive preprocessor like Sci-Kit Learn's StandardScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional API\n",
    "\n",
    "The same thing can be achived in a different way using the functional API.\n",
    "\n",
    "Here an input is defined and then passed to the hidden layers, which in turn pass themself to the next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "\n",
    "input = Input(shape=(28,28))\n",
    "\n",
    "x = Flatten()(input)\n",
    "\n",
    "x = Dense(18, activation='relu')(x)\n",
    "\n",
    "predictions = Dense(10,activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can then be defined from these variable like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "func_model = Model(inputs = input, outputs = predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple inputs and outputs\n",
    "\n",
    "3 inputs and 2 outputs. Simple model\n",
    "\n",
    "Inputs:\n",
    "- `temp_train`\n",
    "- `nocc_train`\n",
    "- `lumbp_train`\n",
    "\n",
    "Outputs:\n",
    "- `out1_train`\n",
    "- `out2_train`\n",
    "\n",
    "Things to note:\n",
    "- `inputs` in the model is a list of the `Input()` parts\n",
    "- `concatentate` is used with the input list to provide input to the next layers of the model\n",
    "- `outputs` in the model is a list of the output layers\n",
    "- When compiling use dictionary to set loss and metrics to each output\n",
    "    - or lists `['binary_crossentropy','binary_crossentropy']`\n",
    "- When fitting, for the inputs/outputs either:\n",
    "    - provide a list of the inputs `[temp_train,nocc_train, lumbp_train]`\n",
    "    - give as a dict `{'layer_name':variable_name}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "## Functional: multiple inputs\n",
    "# N.B. lowercase 'c' concatenate\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Input, Dense, concatenate\n",
    "input_shape=(1,)\n",
    "\n",
    "# get individual inputs\n",
    "inputs_temp = Input(shape=input_shape,name='temp')\n",
    "inputs_nocc = Input(shape=input_shape,name='nocc')\n",
    "inputs_lumbp = Input(shape=input_shape,name='lumbp')\n",
    "\n",
    "# combine them\n",
    "input_list = [inputs_temp,inputs_nocc,inputs_lumbp]\n",
    "input_layer =concatenate(input_list)\n",
    "\n",
    "# add inputs to the model for two outputs\n",
    "output_pred1  = Dense(2,activation='sigmoid',name='out_1')(input_layer)\n",
    "output_pred2 = Dense(2,activation='sigmoid',name='out_2')(input_layer)\n",
    "\n",
    "# output layer\n",
    "output_list = [output_pred1, output_pred2]\n",
    "\n",
    "# create the model object\n",
    "model = tf.keras.Model(inputs=input_list, outputs=output_list )\n",
    "\n",
    "# show the model\n",
    "model.summary()\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "        optimizer='SGD',\n",
    "        loss={'out_1':'binary_crossentropy',\n",
    "              'out_2':'binary_crossentropy'},\n",
    "        metrics={'out_1':['accuracy'],\n",
    "                 'out_2':['accuracy']},\n",
    "        loss_weights=[1,0.2]\n",
    "        )\n",
    "\n",
    "tf.keras.utils.plot_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Define training inputs and outputs\n",
    "inputs_train = {'temp': temp_train, 'nocc': nocc_train, 'lumbp': lumbp_train}\n",
    "outputs_train = {'out_1': out1_train, 'out_2': out2_train}\n",
    "\n",
    "# fit the model\n",
    "model.fit(inputs_train,outputs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Models that can be used found here https://www.tensorflow.org/api_docs/python/tf/keras/applications\n",
    "\n",
    "> Keras Applications are premade architectures with pre-trained weights.\n",
    "\n",
    "## Inception (images)\n",
    "\n",
    "1. Load the model pre-trained weights\n",
    "1. Import the model architecture\n",
    "1. Give the model the input shape for data \n",
    "1. Load the weights into the model\n",
    "1. Freeze all the layers\n",
    "1. Pick out the front part of the model, as the layers to the end are more specialized\n",
    "1. Add extra layers to the model that can be fitted to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# 1- Download the inception v3 weights\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
    "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
    "\n",
    "# 2- Import the inception model  \n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "# Create an instance of the inception model from the local pre-trained weights\n",
    "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "# 3- create the model and load in the weights\n",
    "pre_trained_model = InceptionV3(input_shape = (150, 150, 3),\n",
    "                                  include_top = False, \n",
    "                                  weights = None) \n",
    "\n",
    "# 4- load weights into the model\n",
    "pre_trained_model.load_weights(local_weights_file)\n",
    "\n",
    "# 5- Make all the layers in the pre-trained model non-trainable\n",
    "for layers in pre_trained_model.layers:\n",
    "    layers.trainable = False\n",
    "    \n",
    "# 6- Pick out part of the model\n",
    "last_desired_layer = pre_trained_model.get_layer('mixed7')    \n",
    "last_output = last_desired_layer.output\n",
    "\n",
    "# 7- Add extra layers to the model\n",
    "\n",
    "# Flatten the output layer to 1 dimension\n",
    "x = layers.Flatten()(last_output)  \n",
    "\n",
    "# Add a fully connected layer with 1024 hidden units and ReLU activation\n",
    "x = layers.Dense(1024,activation='relu')(x)\n",
    "# Add a dropout rate of 0.2\n",
    "x = layers.Dropout(0.2)(x)  \n",
    "# Add a final sigmoid layer for classification\n",
    "x = layers.Dense(1,activation='sigmoid')(x) \n",
    "\n",
    "# Create the complete model by using the Model class\n",
    "model = Model(inputs=pre_trained_model.input, outputs=x)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = RMSprop(learning_rate=0.0001), \n",
    "            loss = 'binary_crossentropy',\n",
    "            metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary / info of model\n",
    "\n",
    "`model.summary()`\n",
    "\n",
    "Get summary of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class or Function - Metrics and Losses\n",
    "\n",
    "In general, classes use camel formatting `CategoricalAccuracy` whereas function use underscores and lower case `categorical_accuracy` and sometimes initials `MAE`\n",
    "\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low level handling of metrics\n",
    "\n",
    "- `metric.update_state()` to accumulate metric stats after each batch\n",
    "- `metric.result` get current value of metric to display\n",
    "- `metric.reset_state()` to reset metric value typically at the end of epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical: Binary versus Multiple\n",
    "\n",
    "For categorical data there is a slight difference between if there are only 2 categories or more.\n",
    "\n",
    "Going from binary to multiple:\n",
    "- We need to change activation in model from sigmoid to softmax in final Dense layer\n",
    "- Change loss function from `binary_crossentropy` to `categorical_crossentropy` in compile\n",
    "- Making data one-hot encoded, i.e. columns for each outcome\n",
    "    - Or use `SparseCategoricalCrossentropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model_binary = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(512//8,activation='relu',input_shape=[input_shape]),\n",
    "        tf.keras.layers.Dense(1,activation='sigmoid')        \n",
    "    ])\n",
    "\n",
    "model_multi = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(512//8,activation='relu',input_shape=[input_shape]),\n",
    "        tf.keras.layers.Dense(4,activation='softmax')        \n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model_binary.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_multi.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encoding method\n",
    "\n",
    "y_binary =[1,0,0,0,0,0,1,1,1,1,0,1,0,1]\n",
    "\n",
    "y_multi=[1,2,4,6,1,3,4,2,4,2,5,2,1,4,2,1]\n",
    "y_multi=tf.keras.utils.to_categorical(y_multi)\n",
    "y_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, with output data like [0, 1, 4, 0, 2, 3, 3, 0, ...]\n",
    "\n",
    "use:\n",
    "- `SparseCategoricalCrossentropy(from_logits=True)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "                 loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                 optimizer='adam',\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate\n",
    "\n",
    "Find the best learning rate by using [`callbacks`](https://thomashsimm.com/tensorflow/2022/09/28/Tensorflow.html#Callbacks)\n",
    "\n",
    "The learning rate to use on the data below would be where the loss is low (y-axis) but not too close to where it increases or is unstable.\n",
    "\n",
    "So for this on the downward part of the curve between 10E-6 and 10E-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Set the learning rate scheduler\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-8 * 10**(epoch / 20) )\n",
    "    \n",
    "# Set the training parameters\n",
    "model_tune.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    \n",
    "# train the model\n",
    "history = model_tune.fit(dataset, epochs=100, callbacks=[lr_schedule])\n",
    "\n",
    "# plot the results\n",
    "# Define the learning rate array\n",
    "lrs = 1e-8 * (10 ** (np.arange(100) / 20))\n",
    "\n",
    "# Plot the loss in log scale\n",
    "plt.semilogx(lrs, history.history[\"loss\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ghtop_images/learningrate_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `lambda` functions\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda\n",
    "\n",
    "\n",
    "`tf.keras.layers.Lambda(\n",
    "    function, output_shape=None, mask=None, arguments=None, **kwargs\n",
    ")`\n",
    "\n",
    "Add a function that works on the data within the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# expand the dimensions\n",
    "tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n",
    "                      input_shape=[window_size])\n",
    "\n",
    "# make the output larger (can be useful if predicting to large values, but previous layer have activation function so values are close to 1)\n",
    "tf.keras.layers.Lambda(lambda x: x * 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Force CPU/GPU\n",
    "\n",
    "https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# Check available CPU/GPU devices\n",
    "\n",
    "print(tf.config.list_physical_devices('CPU'))\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "with tf.device(\"CPU:0\"):\n",
    "    model.fit(....)\n",
    "    \n",
    "with tf.device(\"GPU:0\"):\n",
    "    model.fit(....)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Page 2](https://thomashsimm.com/tensorflow/2022/10/07/Tensorflow-2.html)\n",
    "# [Datasets, Tensors, Optimization](https://thomashsimm.com/tensorflow/2022/10/07/Tensorflow-2.html)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
