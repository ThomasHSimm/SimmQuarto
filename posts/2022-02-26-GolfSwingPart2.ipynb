{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Golf Swing Part II- Separating Swing Positions\n",
    "> Using Python to seperate golf swing into parts of the swing\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Neural networks,PyTourch, Golf Swing, Python]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ghtop_images/header2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In a previous part [Part 1](https://thomashsimm.com/jupyter/2021/12/01/GolfPos1FastAI.html) a neural network model was used to find positions on the body during a golf swing. This work used images taken from videos of golf swing (analysed using the code below by the authors listed) because it is often easier to work with images rather than videos.\n",
    "\n",
    "But to get images of the golf swing to analyse it can be useful to get them at different parts of the golf swing. This is what this part does.\n",
    "\n",
    "Taken from https://github.com/wmcnally/golfdb and shown in the paper here https://arxiv.org/abs/1903.06528\n",
    "\n",
    "    [Ref Paper] McNally, William, et al. \"Golfdb: A video database for golf swing sequencing.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. 2019\n",
    "\n",
    "The code separates the golf swing into a number of different segments based on body and golf club positions.\n",
    "\n",
    "This code can be run on kaggle here https://www.kaggle.com/thomassimm/golfdb-lessimports\n",
    "\n",
    "The **input** is an mp3 file of a golf swing\n",
    "\n",
    "The **ouput** is a series of images at different parts of the golf swing\n",
    "\n",
    "\n",
    "![](ghtop_images/golfSep2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Code\n",
    "\n",
    "\n",
    "### Specify the file to use\n",
    "\n",
    "Add downloaded directory (not always necsessary) and specify the video file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "!cp -r ../input/golfdb3/* ./\n",
    "\n",
    "stra='../input/golfdb3/test_video.mp4'\n",
    "stra='../input/golfdb2/golfdb/data/videos_160/1017.mp4'\n",
    "stra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports, classes and defs\n",
    "\n",
    "Some imports. Neural nets using Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "# from eval import ToTensor, Normalize\n",
    "# from model import EventDetector\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following classes and definitions are taken from the files in the GitHub directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "class SampleVideo(Dataset):\n",
    "    def __init__(self, path, input_size=160, transform=None):\n",
    "        self.path = path\n",
    "        self.input_size = input_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cap = cv2.VideoCapture(self.path)\n",
    "        frame_size = [cap.get(cv2.CAP_PROP_FRAME_HEIGHT), cap.get(cv2.CAP_PROP_FRAME_WIDTH)]\n",
    "        ratio = self.input_size / max(frame_size)\n",
    "        new_size = tuple([int(x * ratio) for x in frame_size])\n",
    "        delta_w = self.input_size - new_size[1]\n",
    "        delta_h = self.input_size - new_size[0]\n",
    "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "\n",
    "        # preprocess and return frames\n",
    "        images = []\n",
    "        for pos in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n",
    "            _, img = cap.read()\n",
    "            resized = cv2.resize(img, (new_size[1], new_size[0]))\n",
    "            b_img = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT,\n",
    "                                       value=[0.406 * 255, 0.456 * 255, 0.485 * 255])  # ImageNet means (BGR)\n",
    "\n",
    "            b_img_rgb = cv2.cvtColor(b_img, cv2.COLOR_BGR2RGB)\n",
    "            images.append(b_img_rgb)\n",
    "        cap.release()\n",
    "        labels = np.zeros(len(images)) # only for compatibility with transforms\n",
    "        sample = {'images': np.asarray(images), 'labels': np.asarray(labels)}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        images, labels = sample['images'], sample['labels']\n",
    "        images = images.transpose((0, 3, 1, 2))\n",
    "        return {'images': torch.from_numpy(images).float().div(255.),\n",
    "                'labels': torch.from_numpy(labels).long()}\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = torch.tensor(mean, dtype=torch.float32)\n",
    "        self.std = torch.tensor(std, dtype=torch.float32)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        images, labels = sample['images'], sample['labels']\n",
    "        images.sub_(self.mean[None, :, None, None]).div_(self.std[None, :, None, None])\n",
    "        return {'images': images, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\"\"\"\n",
    "https://github.com/tonylins/pytorch-mobilenet-v2\n",
    "\"\"\"\n",
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        min_depth = 16\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        interverted_residual_setting = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "\n",
    "        # building first layer\n",
    "        assert input_size % 32 == 0\n",
    "        input_channel = int(input_channel * width_mult) if width_mult >= 1.0 else input_channel\n",
    "        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = max(int(c * width_mult), min_depth)\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n",
    "                else:\n",
    "                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, n_class),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean(3).mean(2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class EventDetector(nn.Module):\n",
    "    def __init__(self, pretrain, width_mult, lstm_layers, lstm_hidden, bidirectional=True, dropout=True):\n",
    "        super(EventDetector, self).__init__()\n",
    "        self.width_mult = width_mult\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.lstm_hidden = lstm_hidden\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = dropout\n",
    "\n",
    "        net = MobileNetV2(width_mult=width_mult)\n",
    "        state_dict_mobilenet = torch.load('mobilenet_v2.pth.tar')\n",
    "        if pretrain:\n",
    "            net.load_state_dict(state_dict_mobilenet)\n",
    "\n",
    "        self.cnn = nn.Sequential(*list(net.children())[0][:19])\n",
    "        self.rnn = nn.LSTM(int(1280*width_mult if width_mult > 1.0 else 1280),\n",
    "                           self.lstm_hidden, self.lstm_layers,\n",
    "                           batch_first=True, bidirectional=bidirectional)\n",
    "        if self.bidirectional:\n",
    "            self.lin = nn.Linear(2*self.lstm_hidden, 9)\n",
    "        else:\n",
    "            self.lin = nn.Linear(self.lstm_hidden, 9)\n",
    "        if self.dropout:\n",
    "            self.drop = nn.Dropout(0.5)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.bidirectional:\n",
    "            return (Variable(torch.zeros(2*self.lstm_layers, batch_size, self.lstm_hidden).cuda(), requires_grad=True),\n",
    "                    Variable(torch.zeros(2*self.lstm_layers, batch_size, self.lstm_hidden).cuda(), requires_grad=True))\n",
    "        else:\n",
    "            return (Variable(torch.zeros(self.lstm_layers, batch_size, self.lstm_hidden).cuda(), requires_grad=True),\n",
    "                    Variable(torch.zeros(self.lstm_layers, batch_size, self.lstm_hidden).cuda(), requires_grad=True))\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # CNN forward\n",
    "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
    "        c_out = self.cnn(c_in)\n",
    "        c_out = c_out.mean(3).mean(2)\n",
    "        if self.dropout:\n",
    "            c_out = self.drop(c_out)\n",
    "\n",
    "        # LSTM forward\n",
    "        r_in = c_out.view(batch_size, timesteps, -1)\n",
    "        r_out, states = self.rnn(r_in, self.hidden)\n",
    "        out = self.lin(r_out)\n",
    "        out = out.view(batch_size*timesteps,9)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "seq_length=64\n",
    "\n",
    "ds = SampleVideo(stra, transform=transforms.Compose([ToTensor(),\n",
    "                                Normalize([0.485, 0.456, 0.406],\n",
    "                                          [0.229, 0.224, 0.225])]))\n",
    "\n",
    "dl = DataLoader(ds, batch_size=1, shuffle=False, drop_last=False)\n",
    "\n",
    "model = EventDetector(pretrain=True,\n",
    "                      width_mult=1.,\n",
    "                      lstm_layers=1,\n",
    "                      lstm_hidden=256,\n",
    "                      bidirectional=True,\n",
    "                      dropout=False)\n",
    "try:\n",
    "    save_dict = torch.load('models/swingnet_1800.pth.tar')\n",
    "except:\n",
    "    print(\"Model weights not found. Download model weights and place in 'models' folder. See README for instructions\")\n",
    "    \n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "model.load_state_dict(save_dict['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Loaded model weights\")\n",
    "\n",
    "print('Testing...')\n",
    "for sample in dl:\n",
    "    images = sample['images']\n",
    "    # full samples do not fit into GPU memory so evaluate sample in 'seq_length' batches\n",
    "    batch = 0\n",
    "    while batch * seq_length < images.shape[1]:\n",
    "        if (batch + 1) * seq_length > images.shape[1]:\n",
    "            image_batch = images[:, batch * seq_length:, :, :, :]\n",
    "        else:\n",
    "            image_batch = images[:, batch * seq_length:(batch + 1) * seq_length, :, :, :]\n",
    "        logits = model(image_batch.cuda())\n",
    "        if batch == 0:\n",
    "            probs = F.softmax(logits.data, dim=1).cpu().numpy()\n",
    "        else:\n",
    "            probs = np.append(probs, F.softmax(logits.data, dim=1).cpu().numpy(), 0)\n",
    "        batch += 1\n",
    "\n",
    "        \n",
    "events = np.argmax(probs, axis=0)[:-1]\n",
    "print('Predicted event frames: {}'.format(events))\n",
    "\n",
    "\n",
    "confidence = []\n",
    "for i, e in enumerate(events):\n",
    "    confidence.append(probs[e, i])\n",
    "print('Confidence: {}'.format([np.round(c, 3) for c in confidence]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "    \n",
    "\n",
    "Using device: cuda\n",
    "\n",
    "Loaded model weights\n",
    "\n",
    "Testing...\n",
    "\n",
    "Predicted event frames: [ 82 121 137 166 189 203 213 245]\n",
    "\n",
    "Confidence: [0.215, 0.376, 0.79, 0.767, 0.827, 0.968, 0.935, 0.247]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "##delte images\n",
    "lsa=os.listdir()\n",
    "fimg=[ ll for ll in lsa if ll.split('.')[-1]=='jpg']\n",
    "# print(fimg)\n",
    "imgs=[os.remove(ff) for ff in fimg]\n",
    "\n",
    "fimg=[ ll for ll in lsa if ll.split('.')[-1]=='jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def createImages(fila,pos,nomS):\n",
    "    ''' \n",
    "    Given a video file location (fila) it will save as images to a folder\n",
    "    Given positions in video (pos) these images from the video are saved\n",
    "    pos is created based on positions of swings\n",
    "    '''\n",
    "    import cv2\n",
    "    cap = cv2.VideoCapture(fila)\n",
    "    eventNom=[0,1,2,3,4,5,6,7]\n",
    "    for i, e in enumerate(events):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, e)\n",
    "        _, img = cap.read()\n",
    "        cv2.imwrite(os.path.join(os.getcwd(),'_'+ nomS+'_'+\"frame{:d}.jpg\".format(eventNom[i])), img)     # save frame as JPG file\n",
    "    \n",
    "    \n",
    "fila=stra\n",
    "pos=events\n",
    "createImages(fila,pos,'10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "lsa=os.listdir()\n",
    "fimg=[ ll for ll in lsa if ll.split('.')[-1]=='jpg']\n",
    "fimg.sort()\n",
    "\n",
    "imgs=[mpimg.imread(ff) for ff in fimg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(stra)\n",
    "\n",
    "\n",
    "# plt.subplot(4,2,1)\n",
    "f, axs = plt.subplots(4,2,figsize=(15,15))\n",
    "for i, e in enumerate(events):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, e)\n",
    "    _, img = cap.read()\n",
    "    plt.subplot(4,2,i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ghtop_images/golfSep.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
