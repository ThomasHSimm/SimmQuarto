{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastAI CheatSheet\n",
    "> Using FastAI\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Neural networks,FastAI, Python]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](ghtop_images/header2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc.\n",
    "\n",
    "### Creating a path\n",
    "\n",
    "Where the data is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# current directory\n",
    "path=Path()\n",
    "\n",
    "Path.BASE_PATH = path\n",
    "\n",
    "# another folder off cwd\n",
    "path = Path('bears')\n",
    "#if doesn't exist can do\n",
    "path.mkdir()\n",
    "\n",
    "# as part of downloading inbuilt data\n",
    "path = untar_data(URLs.IMDB)\n",
    "\n",
    "\n",
    "# When using path to select a subfolder\n",
    "`trains = path/'train'`\n",
    "Or to view\n",
    "`(path/'train').ls()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data \n",
    "\n",
    "`DataLoaders` is a thin class that just stores whatever `DataLoader` objects you pass to it, and makes them available as `train` and `valid`. Although it's a very simple class, it's very important in fastai: it provides the data for your model. The key functionality in `DataLoaders` is provided with just these four lines of code (it has some other minor functionality we'll skip over for now):\n",
    "\n",
    "```python\n",
    "class DataLoaders(GetAttr):\n",
    "    def __init__(self, *loaders): self.loaders = loaders\n",
    "    def __getitem__(self, i): return self.loaders[i]\n",
    "    train,valid = add_props(lambda i,self: self[i])\n",
    "```\n",
    "\n",
    "To turn our downloaded data into a `DataLoaders` object we need to tell fastai at least four things:\n",
    "\n",
    "- What kinds of data we are working with\n",
    "- How to get the list of items\n",
    "- How to label these items\n",
    "- How to create the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataBlocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "dBlock = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock), \n",
    "    get_items=get_image_files, \n",
    "    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
    "    get_y=parent_label,\n",
    "    item_tfms=Resize(128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`blocks` what format is the data?\n",
    "- `ImageBlock`\n",
    "- `CategoryBlock`\n",
    "- `TextBlock`\n",
    "\n",
    "`get_items` takes a function that that gives the list of all data (images/text etc) \n",
    "- `get_image_files`\n",
    "- `partial(get_text_files, folders=['train', 'test', 'unsup'])`\n",
    "\n",
    "`splitter` how data is plit into test and validation sets\n",
    "- `RandomSplitter(valid_pct=0.2, seed=42)` randomly\n",
    "- `def splitter(df):\n",
    "    train = df.index[~df['is_valid']].tolist()\n",
    "    valid = df.index[df['is_valid']].tolist()\n",
    "    return train,valid` use a function this one uses data frame\n",
    "    \n",
    "`get_x` and `get_y` get the independent (x) and dependent variables (y) takes a function that provides labels for the data\n",
    "- `parent_label` gets the name of the parent folder e.g. when doing categorical data put rabbits in 'rabbits' folder and horses in 'horses' folder\n",
    "- `def get_y(r): return r['labels'].split(' ')`\n",
    "\n",
    "`item_tfms` runs on individual items- allows for data augmentation, making images the same size etc\n",
    "- `Resize(128)` resize all images to 128\n",
    "- `Resize(128, ResizeMethod.Pad, pad_mode='zeros'` resize with zeros\n",
    "\n",
    "`batch_tfms` similar to the above but apply to all batch\n",
    "- `[*aug_transforms(size=size, min_scale=0.75),`\n",
    "   `Normalize.from_stats(*imagenet_stats)]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders\n",
    "\n",
    "The data block is like a template for creating a DataLoaders. We still need to tell fastai the actual source of our data—in this case, the path where the images can be found along with some other details.\n",
    "\n",
    "A `DataLoaders` includes validation and training `DataLoader`s. `DataLoader` is a class that provides batches of a few items at a time to the GPU. When you loop through a `DataLoader` fastai will give you 64 (by default) items at a time, all stacked up into a single tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "dls = dBlock.dataloaders(path, \n",
    "            path=path, \n",
    "            bs=128, \n",
    "            seq_len=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`after_item` applied after each item equivalent of `item_tfms`\n",
    "\n",
    "`before_batch` applied on list of items before they're collated\n",
    "\n",
    "`after_batch` applied on the batch as a whole after construction- equivalent to `batch_tfms`\n",
    "\n",
    "`bs` batch size\n",
    "\n",
    "`seq_len`\n",
    "\n",
    "`path` path to data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different models\n",
    "\n",
    "#### Vision\n",
    "\n",
    "`from fastai.vision import models`\n",
    "\n",
    "`learn = cnn_learner(data, models.resnet18, metrics=accuracy)`\n",
    "\n",
    "Torchvision models\n",
    "\n",
    "- `resnet18`, `resnet34`, `resnet50`, `resnet101`, `resnet152`\n",
    "- `squeezenet1_0`, `squeezenet1_1`\n",
    "- `densenet121`, `densenet169`, `densenet201`, `densenet161`\n",
    "- `vgg16_bn`, `vgg19_bn`\n",
    "- `alexnet`\n",
    "Others\n",
    "- `Darknet` \n",
    "- `unet`\n",
    "\n",
    "#### Text\n",
    "\n",
    "`from fastai.text import *` \n",
    "\n",
    "`learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)`\n",
    "\n",
    "Or for classification\n",
    "\n",
    "`learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)`\n",
    "\n",
    "#### Tabular\n",
    "\n",
    "`from fastai.tabular import * `\n",
    "\n",
    "`learn = tabular_learner(data, layers=[200,100], emb_szs={'native-country': 10}, metrics=accuracy)`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the model we have a few options:\n",
    "\n",
    "- `learn.fit(10,lr=4e-3)`\n",
    "\n",
    "- `learn.fit_one_cycle()`\n",
    "\n",
    "- `learn.fine_tune(10, base_lr=1e-3, freeze_epochs=7)`\n",
    "\n",
    "- `learn.fine_tune(15, lr)`\n",
    "\n",
    "\n",
    "FastAI adds an extra 2 layers on the end of neural network when doing **transfer learning**, these can then be fitted using fine_tune. It is recommended to do a few fits frozen before unfreezing. This is normally the best option for transfer learning.\n",
    "\n",
    "But the other ones can be used. In general fit can be more unstable and lead to bigger losses, but can be useful if fine_tune is not bringing losses down.\n",
    "\n",
    "https://forums.fast.ai/t/fine-tune-vs-fit-one-cycle/66029/6\n",
    "\n",
    "    fit_one_cycle = New Model\n",
    "    \n",
    "    fine_tuning = with Transfer Learning?\n",
    "\n",
    "    I’d say yes but with a very strong but, only because it’s easy to fall into a trap that way. fine_tuning is geared towards transfer learning specifically, but you can also just do fit_one_cycle as well! (Or flat_cos).\n",
    "\n",
    "    For beginners it’s a great starting fit function (and advanced too), but also don’t forget that you can then\n",
    "    \n",
    "    \n",
    "An alternative to `fine_tuning` with transfer learning is to specify which layers are frozen:\n",
    "\n",
    "Unfreeze layers, to freeze all except the last two parameter groups use `freeze_to`:\n",
    "\n",
    "`learn.freeze_to(-2)`\n",
    "\n",
    "`learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))`\n",
    "\n",
    "And unfreeze a bit more\n",
    "\n",
    "`learn.freeze_to(-3)`\n",
    "\n",
    "Or unfreeze the whole model\n",
    "\n",
    "`learn.unfreeze`\n",
    "\n",
    "Can see the difference between `fine_tune` and `fit_one_cycle` from the `fine_tune` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,\n",
    "              pct_start=0.3, div=5.0, **kwargs):\n",
    "    \"Fine tune with `freeze` for `freeze_epochs` then with `unfreeze` from `epochs` using discriminative LR\"\n",
    "    self.freeze()\n",
    "    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n",
    "    base_lr /= 2\n",
    "    self.unfreeze()\n",
    "    self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some other useful bits\n",
    "\n",
    "Find the best learing rate:\n",
    "\n",
    "`learn.lr_find()`\n",
    "\n",
    "![](ghtop_images/GOLF5.png)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
