<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.258">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ThomasHSimm – tensorflow-images</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="..//posts/Picture3.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">ThomasHSimm</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/ThomasHSimm"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/ThomasHSimm"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content"><p><img src="../posts/header2.png" height="200"></p>



<section id="tensorflow-images-cheat-sheet" class="level1">
<h1>TensorFlow Images cheat sheet</h1>
<blockquote class="blockquote">
<p>Some tips for tensorflow and keras in Image Processing</p>
</blockquote>
<ul>
<li>toc: true</li>
<li>badges: true</li>
<li>comments: true</li>
<li>categories: [tensorflow, Images]</li>
<li>image: ghtop_images/header2.png</li>
</ul>
<p><img src="ghtop_images/header2.png" class="img-fluid"></p>
</section>
<section id="basic-implementation" class="level1">
<h1>Basic Implementation</h1>
<div class="cell" data-run_control="{&quot;frozen&quot;:true}">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> tf.keras.models.Sequential([ </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>          tf.keras.layers.Convolution2D( <span class="dv">64</span>,(<span class="dv">3</span>,<span class="dv">3</span>),activation<span class="op">=</span><span class="st">'relu'</span>,input_shape<span class="op">=</span>(<span class="dv">28</span>,<span class="dv">28</span>,<span class="dv">1</span>) ),</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>          tf.keras.layers.MaxPool2D(<span class="dv">2</span>,<span class="dv">2</span>),</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>          tf.keras.layers.Flatten(),</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>          tf.keras.layers.Dense(<span class="dv">256</span><span class="op">//</span><span class="dv">2</span>,activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>          tf.keras.layers.Dense(<span class="dv">1</span>,activation<span class="op">=</span><span class="st">'sigmoid'</span>) ])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="data-augmentation" class="level1">
<h1>Data augmentation</h1>
<p>https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator</p>
<p>Before the data is presented to the model, data augmentation can be performed on the data.</p>
<p>The advantage of this is that it can effecitively create new data for the model and hopefully reduce overfitting. The most common use of this is for images but methods also exist for other data types. For example, for text data we could pass the data through a translator and back again.</p>
<div class="cell" data-run_control="{&quot;frozen&quot;:true}" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.preprocessing.image <span class="im">import</span> ImageDataGenerator</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>train_datagen <span class="op">=</span> ImageDataGenerator(rescale<span class="op">=</span><span class="fl">1.</span><span class="op">/</span><span class="fl">255.</span>,</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>                                     rotation_range<span class="op">=</span><span class="dv">40</span>,</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>                                     width_shift_range<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>                                     height_shift_range<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>                                     shear_range<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>                                     zoom_range<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>                                     horizontal_flip<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>                                     fill_mode<span class="op">=</span><span class="st">"nearest"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="extracting-information-from-layers-of-a-model--images" class="level1">
<h1>Extracting information from layers of a model- Images</h1>
<ul>
<li>Give the image to the model</li>
<li>Each part is a different layer, and within it the different neurons</li>
<li>Alternatively the same can be done by setting the output of the model to be the mid-level layer
<ul>
<li><code>model_extract = Model(inputs = model.input,</code> <code>outputs=model.get_layer('block4_conv4').output)</code></li>
</ul></li>
</ul>
<div class="cell" data-run_control="{&quot;frozen&quot;:true}">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualise the input channels</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>img_path <span class="op">=</span> <span class="st">'data/cool_cat.jpg'</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> image.img_to_array(img)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>f1 <span class="op">=</span> model(x)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_layers(f1,layer_no<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(f1[layer_no].shape)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>,<span class="dv">15</span>))</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    random_9 <span class="op">=</span> np.random.randint(<span class="dv">0</span>,f1[layer_no].shape[<span class="op">-</span><span class="dv">1</span>],<span class="dv">30</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    nnn<span class="op">=</span><span class="dv">0</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> nn <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="dv">30</span>):</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        n<span class="op">=</span>random_9[nn]</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        plt.subplot(<span class="dv">3</span>,<span class="dv">3</span>,nnn<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>            plt.imshow( f1[layer_no][<span class="dv">0</span>,:,:,n])</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>            plt.axis(<span class="st">'off'</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>            plt.title(<span class="ss">f"layer no: </span><span class="sc">{</span>layer_no<span class="sc">}</span><span class="ss">, part of layer: </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>            nnn<span class="op">=</span>nnn<span class="op">+</span><span class="dv">1</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span>:</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'failed </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> nnn<span class="op">==</span><span class="dv">9</span>:</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">'breaking'</span>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>plot_layers(f1,layer_no<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img src="ghtop_images/lion_layer1.png" class="img-fluid"></p>
</section>
<section id="load-images" class="level1">
<h1>Load images</h1>
<section id="load-from-directory" class="level2">
<h2 class="anchored" data-anchor-id="load-from-directory">Load from directory</h2>
<p>If files are in folders can use <code>flow_from_directory</code> the files would need to be separated by class and training/validation as follows for a classification</p>
<p>e.g.&nbsp;files in folders like this:</p>
<ul>
<li><code>/tmp/cats-v-dogs/validation/cats</code></li>
<li><code>/tmp/cats-v-dogs/validation/dogs</code></li>
<li><code>/tmp/cats-v-dogs/training/cats</code></li>
<li>`/tmp/cats-v-dogs/training/dogs</li>
</ul>
<div class="cell" data-run_control="{&quot;frozen&quot;:true}">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>train_datagen <span class="op">=</span> ImageDataGenerator()</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>train_generator <span class="op">=</span> train_datagen.flow_from_directory(directory<span class="op">=</span>TRAINING_DIR,</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>                                                      batch_size<span class="op">=</span><span class="dv">256</span>,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>                                                      class_mode<span class="op">=</span><span class="st">'binary'</span>,</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>                                                      target_size<span class="op">=</span>(<span class="dv">150</span>, <span class="dv">150</span>))</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Test your generators</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>train_generator, validation_generator <span class="op">=</span> train_val_generators(TRAINING_DIR, VALIDATION_DIR)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Put in the fit</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>model.fit(train_generator,</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>                    epochs<span class="op">=</span><span class="dv">15</span>,</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>                    verbose<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>validation_generator)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="classification" class="level1">
<h1>Classification</h1>
<p>Some terms:</p>
<ul>
<li>Multi-class Classification (or binary classification a subset of this)
<ul>
<li>e.g.&nbsp;Find whether image is 1, 2, 3, 4… in MNIST</li>
</ul></li>
<li>Multi-label Classification
<ul>
<li>Can find multiple objects in the same image</li>
<li>e.g.&nbsp;image has cats AND dogs in it</li>
</ul></li>
<li>Object Localization
<ul>
<li>Where is the object in an image</li>
</ul></li>
<li>Object detection
<ul>
<li>Combines object localization and multi-label classification</li>
<li>e.g.&nbsp;here in the image is the cat, here is the dog, here is the person</li>
<li>has confidence scores</li>
<li>has bounding boxes</li>
<li>Algorithms include
<ul>
<li>R-CNN</li>
<li>Faster-RCNN</li>
<li>YOLO</li>
<li>SSD</li>
</ul></li>
</ul></li>
<li>Image segmentation
<ul>
<li>instead of bounding box, image segmentation figures out the pixels belonging to an object</li>
<li>sementatic segmentation - groups all instances of same type together
<ul>
<li>i.e.&nbsp;group of people classed as one object</li>
<li>models include Fully Convoluted Neural Networks, U-Net, DeepLAb</li>
</ul></li>
<li>instance segmentation- groups by instances of the class
<ul>
<li>i.e.&nbsp;each person in group is separate</li>
<li>Mask R-CNN</li>
</ul></li>
</ul></li>
<li>iou- intersection over union
<ul>
<li>metric used to define how good object localization is</li>
<li>intersection = area where actual and predicted bounding boxes overlap- i.e.&nbsp;their intersection</li>
<li>union = total area covered by both boxes</li>
<li>metric iou = intersection / union</li>
<li>so if iou = 0 no overlap</li>
<li>if iou ==1 complete overlap</li>
</ul></li>
</ul>
<p>References:</p>
<ul>
<li><p><a href="https://people.eecs.berkeley.edu/~shelhamer/data/fcn.pdf">Fully Convolutional Networks for Semantic Segmentation</a> (Long, Shelhamer &amp; Darrell, 2014)</p></li>
<li><p><a href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/">U-Net: Convolutional Networks for Biomedical Image Segmentation</a> (Ronneberger, Fischer &amp; Brox, 2015)</p></li>
<li><p><a href="http://liangchiehchen.com/projects/DeepLab.html">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a> (Chen, Papandreou, Kokkinos, Murphy, and Yuille, 2016)</p></li>
<li><p><a href="https://arxiv.org/abs/1703.06870">Mask R-CNN</a> (He, Gkioxari, Dollár &amp; Girshick, 2017)</p></li>
</ul>
</section>
<section id="transfer-learning" class="level1">
<h1>Transfer Learning</h1>
<p>Models that can be used found here https://www.tensorflow.org/api_docs/python/tf/keras/applications</p>
<blockquote class="blockquote">
<p>Keras Applications are premade architectures with pre-trained weights.</p>
</blockquote>
<ul>
<li>Use pre-existing models trained on <em>similar</em> data
<ul>
<li>Task being applied are called <em>downstream task</em></li>
</ul></li>
<li>Use model and layers and weights of pre-trained model</li>
<li>Will be trained on many examples and have learned various features</li>
</ul>
<p>Why? - Reduces computation - Improve model performance</p>
<p>Methods:</p>
<p>Take the pre-trained model and add on additional Dense Layers that will be trained at the end - The pre-trained model’s final layers are removed as they may be overspecialized for tasks not relevent to our use - all the pre-trained model’s layers used are <strong>freezed</strong> and not involved in training</p>
<p>Alternatively, we could allow the pre-trained layers to be trained whilst using the pre-trained weights.</p>
<section id="an-example-of-transfer-learning-on-images-with-inception" class="level2">
<h2 class="anchored" data-anchor-id="an-example-of-transfer-learning-on-images-with-inception">An example of transfer learning on images with Inception</h2>
<ol type="1">
<li>Load the model pre-trained weights</li>
<li>Import the model architecture</li>
<li>Give the model the input shape for data</li>
</ol>
<p><code>pre_trained_model = InceptionV3(input_shape = (150, 150, 3),                                   include_top = False,                                    weights = None)</code></p>
<p>Better to keep the same shape as the model uses and change your data to match it than to change <code>input_shape</code> to match your data.</p>
<p><code>include_top=False</code> removes top most layer of the model- the output layer</p>
<p><code>weights=None</code> just uses the model architecture- note the weights are loaded on a later line</p>
<ol start="4" type="1">
<li>Load the weights into the model</li>
<li>Freeze all the layers</li>
<li>Pick out the front part of the model, as the layers to the end are more specialized</li>
<li>Add extra layers to the model that can be fitted to</li>
</ol>
<p>Note this uses the functional API</p>
<ol start="8" type="1">
<li>Match the image size of our images to that needed by the model</li>
</ol>
<p>Our model expects 150X150X3 but our data is 50X50. So we need to multiply our image size by 3. This is done by the <code>UpSampling2D</code> layer</p>
<p><code>model = tf.keras.layers.UpSampling2D(size=(3,3))(model)</code></p>
<p>or use resize if image is bigger <code>tf.image.resize(image, (150, 150,))</code></p>
<div class="cell" data-run_control="{&quot;frozen&quot;:true}" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1- Download the inception v3 weights</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>wget <span class="op">--</span>no<span class="op">-</span>check<span class="op">-</span>certificate <span class="op">\</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    https:<span class="op">//</span>storage.googleapis.com<span class="op">/</span>mledu<span class="op">-</span>datasets<span class="op">/</span>inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 <span class="op">\</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="op">-</span>O <span class="op">/</span>tmp<span class="op">/</span>inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 2- Import the inception model  </span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.applications.inception_v3 <span class="im">import</span> InceptionV3</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an instance of the inception model from the local pre-trained weights</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>local_weights_file <span class="op">=</span> <span class="st">'/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 3- create the model and load in the weights</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>pre_trained_model <span class="op">=</span> InceptionV3(input_shape <span class="op">=</span> (<span class="dv">150</span>, <span class="dv">150</span>, <span class="dv">3</span>),</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>                                  include_top <span class="op">=</span> <span class="va">False</span>, </span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>                                  weights <span class="op">=</span> <span class="va">None</span>) </span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 4- load weights into the model</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>pre_trained_model.load_weights(local_weights_file)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 5- Make all the layers in the pre-trained model non-trainable</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> layers <span class="kw">in</span> pre_trained_model.layers:</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    layers.trainable <span class="op">=</span> <span class="va">False</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 6- Pick out part of the model</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>last_desired_layer <span class="op">=</span> pre_trained_model.get_layer(<span class="st">'mixed7'</span>)    </span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>last_output <span class="op">=</span> last_desired_layer.output</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 7- Add extra layers to the model</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten the output layer to 1 dimension</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Flatten()(last_output)  </span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a fully connected layer with 1024 hidden units and ReLU activation</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Dense(<span class="dv">1024</span>,activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a dropout rate of 0.2</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(x)  </span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a final sigmoid layer for classification</span></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> layers.Dense(<span class="dv">1</span>,activation<span class="op">=</span><span class="st">'sigmoid'</span>)(x) </span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a><span class="co"># 8. Increase input image sizes to match that needed by model using </span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a><span class="co"># a layer before the existing model starts</span></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> tf.keras.layers.UpSampling2D(size<span class="op">=</span>(<span class="dv">3</span>,<span class="dv">3</span>))(model)</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the complete model by using the Model class</span></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(inputs<span class="op">=</span>pre_trained_model.<span class="bu">input</span>, outputs<span class="op">=</span>x)</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the model</span></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer <span class="op">=</span> RMSprop(learning_rate<span class="op">=</span><span class="fl">0.0001</span>), </span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> <span class="st">'binary_crossentropy'</span>,</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>            metrics <span class="op">=</span> [<span class="st">'accuracy'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="object-detection" class="level1">
<h1>Object Detection</h1>
<p>Two stages: 1. Region proposal - e.g.&nbsp;sliding window 2. Object detection and classification - identifies and classifies objects in that region</p>
<p><a href="https://aws.amazon.com/rekognition/?blog-cards.sort-by=item.additionalFields.createdDate&amp;blog-cards.sort-order=desc">Amazon Rekognition</a></p>
<p><a href="https://cloud.ibm.com/catalog#services">PowerAI</a></p>
<p><a href="https://developer.nvidia.com/digits">DIGITS</a></p>
<ul>
<li>Sliding window- a window smaller than the image scans accross the image. Detection is done on each window.
<ul>
<li>image window size and image size can be scaled</li>
<li>multiple boxes could select the same image
<ul>
<li>pick one with highest iou</li>
<li>called non-maximum suppression (nms)</li>
</ul></li>
</ul></li>
</ul>
<p><img src="ghtop_images/slidingbox.png" class="img-fluid"></p>
<ul>
<li>Selective search
<ul>
<li>The algorithm makes guesses about what might contain the objects of interest.</li>
<li>Groups where object is detected are merged to obtain a bounding boxto define the group</li>
<li>A slow approach, but in accuracy can be good.</li>
</ul></li>
</ul>
<p><img src="ghtop_images/selectivesearch.png" class="img-fluid"></p>
<section id="r-cnn" class="level2">
<h2 class="anchored" data-anchor-id="r-cnn">R-CNN</h2>
<p>R-CNN (R=region) a region based CNN to implement selective search with neural networks.</p>
<ul>
<li>Takes input images</li>
<li>Extract regions using selective search method (~2k)</li>
<li>Extract features using CNN from each region
<ul>
<li>warped to match AlexNet inputs</li>
</ul></li>
<li>Classify with support vector machine (SVM) instead of dense layers</li>
<li>Plus regression to get bounding box of images</li>
<li>Very slow &amp; computationally expensive</li>
</ul>
<p><img src="ghtop_images/rcnn.png" class="img-fluid"></p>
<p><a href="https://arxiv.org/abs/1311.2524">Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, “Rich feature hierarchies for accurate object detection and semantic segmentation” 2014</a></p>
</section>
<section id="fast-r-cnn" class="level2">
<h2 class="anchored" data-anchor-id="fast-r-cnn">Fast R-CNN</h2>
<p>The aim was to improve issues above with RCNN.</p>
<ul>
<li>Entire image is fed into the ConvNet
<ul>
<li>no selective search- computationally expensive</li>
<li>this Convnet is trained on finding features</li>
<li>produces a feature map of the image</li>
</ul></li>
<li>Each feature map can then be fed into fully connected dense layer
<ul>
<li>get feature vector of image</li>
</ul></li>
<li>Feature vector fed into layers to do regression and classification</li>
</ul>
<p><img src="ghtop_images/fastRCNN.png" class="img-fluid"></p>
<p><a href="https://arxiv.org/pdf/1504.08083.pdf">Ross Girshick, Fast R-CNN, 2015</a></p>
</section>
<section id="faster-r-cnn" class="level2">
<h2 class="anchored" data-anchor-id="faster-r-cnn">Faster R-CNN</h2>
<ul>
<li>entire image into Convnet</li>
<li>sliding window to find areas of interest</li>
<li>something called a Region Proposed Network is used with data to find and create anchor boxes on image</li>
<li>The cropped and passed to Dense layers for classification and regression.</li>
</ul>
<p><img src="ghtop_images/fasterRCNN.png" class="img-fluid"> <a href="https://pyimagesearch.com/deep-learning-computer-vision-python-book/">Deep Learning for Computer Vision with Python</a></p>
</section>
<section id="object-detection-in-tensorflow" class="level2">
<h2 class="anchored" data-anchor-id="object-detection-in-tensorflow">Object detection in TensorFlow</h2>
<p>https://www.tensorflow.org/hub</p>
<blockquote class="blockquote">
<p>TensorFlow Hub is a repository of trained machine learning models ready for fine-tuning and deployable anywhere. Reuse trained models like BERT and Faster R-CNN with just a few lines of code.</p>
</blockquote>
<p>Copy url from hub page https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1 page for faster rcnn. And copy url is “https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1”</p>
<div class="cell" data-run_control="{&quot;frozen&quot;:true}" data-execution_count="13">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow<span class="op">-</span>hub <span class="im">as</span> hub</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>module_handle <span class="op">=</span> <span class="st">"https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1"</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>detector <span class="op">=</span> hub.load(module_handle).signatures[<span class="st">'default'</span>]</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>An example can be found here https://www.tensorflow.org/hub/tutorials/object_detection</p>
</section>
<section id="object-detection-api" class="level2">
<h2 class="anchored" data-anchor-id="object-detection-api">Object Detection API</h2>
<p>https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_object_detection.ipynb</p>
<p>https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md https://github.com/tensorflow/models/tree/master/research/object_detection https://www.tensorflow.org/guide/checkpoint</p>
</section>
</section>
<section id="useful-bits-of-tf-code" class="level1">
<h1>Useful bits of tf code</h1>
<p><strong>Add batch dimension</strong></p>
<p>Images are expected to be of the form [sample_no, image_X, image_Y, RGB] but if single image the first dimension will be missing</p>
<p><code>img = load_img(downloaded_image_path) print(img.shape)</code></p>
<p>(2592, 3872, 3)</p>
<p><code>img_new = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...] print( img_new.shape)</code></p>
<p>(1, 2592, 3872, 3)</p>
<p><strong>Convert image to a tensor</strong></p>
<p><code>img = tf.image.decode_jpeg(img, channels=3)</code></p>
</section>
<section id="image-tools" class="level1">
<h1>Image tools</h1>
<div class="cell" data-run_control="{&quot;frozen&quot;:true}" data-execution_count="24">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> ImageOps</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> six.moves.urllib.request <span class="im">import</span> urlopen</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> six <span class="im">import</span> BytesIO</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>url_img <span class="op">=</span> <span class="st">'https://upload.wikimedia.org/wikipedia/commons/4/4f/PEOPLE_WAITING_TO_CROSS_A_STREET_IN_TOKYO_2017.jpg'</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># reads the image fetched from the UR</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>image_data <span class="op">=</span> urlopen(url_img).read()</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co"># puts the image data in memory buffer</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>image_data <span class="op">=</span> BytesIO(image_data)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co"># opens the image</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>pil_image <span class="op">=</span> Image.<span class="bu">open</span>(image_data)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co"># resizes the image. will crop if aspect ratio is different.</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>pil_image <span class="op">=</span> ImageOps.fit(pil_image, (<span class="dv">256</span>, <span class="dv">256</span>), Image.ANTIALIAS)    </span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co"># converts to the RGB colorspace</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>pil_image_rgb <span class="op">=</span> pil_image.convert(<span class="st">"RGB"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-run_control="{&quot;frozen&quot;:true}" data-execution_count="25">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(pil_image_rgb)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="va">False</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="2022-10-10-Tensorflow-Images_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>